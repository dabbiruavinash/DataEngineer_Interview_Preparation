covering other topics in databricks classified as bascic,intermdeiate ,pro

1. Delta Lake Interview Questions

Basic Questions:
What is Delta Lake, and how does it differ from traditional data lakes?
What are the key features of Delta Lake?
How does Delta Lake handle ACID transactions in big data processing?
What is schema enforcement, and how does it work in Delta Lake?
What is schema evolution in Delta Lake? How do you handle schema changes?

Intermediate Questions:
How does Delta Lake store data internally? Explain the role of Parquet and transaction logs.
What is Time Travel in Delta Lake, and how do you use it?
How does Delta Lake optimize queries using Z-Ordering and data skipping?
What is the role of OPTIMIZE and VACUUM in Delta Lake?
Explain how Delta Lake prevents data corruption in a multi-user environment.

Advanced Questions (Real-World Scenarios):
How would you incrementally load data into a Delta table?
How does Merge, Update, and Delete work in Delta Lake?
What are the differences between append-only tables and mergeable tables in Delta Lake?
How do you automate the compaction of small files in Delta Lake?
What challenges can arise when using Delta Lake in streaming pipelines, and how do you overcome them?

2. Unity Catalog Interview Questions
Basic Questions:
What is Unity Catalog, and why is it used in Databricks?
How does Unity Catalog improve data governance in Databricks?
What are the different privileges and permissions available in Unity Catalog?
How does RBAC (Role-Based Access Control) work in Unity Catalog?
What is the difference between Unity Catalog and Hive Metastore?

Intermediate Questions:
How do you enable cross-workspace data sharing in Unity Catalog?
What are managed tables vs. external tables in Unity Catalog?
How does column-level security work in Unity Catalog?
What is Data Lineage, and how does Unity Catalog track it?
How do Service Principals and Personal Access Tokens (PATs) work with Unity Catalog?

Advanced Questions (Real-World Scenarios):
How do you set up fine-grained access control for different user roles in Unity Catalog?
How can Unity Catalog be integrated with Azure Purview for data governance?
How would you migrate from Hive Metastore to Unity Catalog?
How do you enforce data masking in Unity Catalog?
How does data versioning work with Unity Catalog?

3. Delta Live Tables (DLT) Interview Questions
Basic Questions:
What is Delta Live Tables (DLT) in Databricks?
How does Delta Live Tables differ from normal Delta Tables?
What are the benefits of using Delta Live Tables for streaming and batch processing?
What is the difference between STREAMING and INCREMENTAL tables in Delta Live Tables?
How do data quality constraints work in Delta Live Tables?

Intermediate Questions:
How does Auto-Maintenance work in Delta Live Tables?
What is the role of Pipelines in Delta Live Tables?
How do you enable Change Data Capture (CDC) using Delta Live Tables?
How do you monitor pipeline failures and logs in Delta Live Tables?
How can Delta Live Tables be integrated with Unity Catalog?

Advanced Questions (Real-World Scenarios):
How would you implement a real-time data ingestion pipeline using Delta Live Tables?
How does Delta Live Tables handle schema evolution in a production environment?
How do you optimize cost and performance in Delta Live Tables?
What are the challenges of using Delta Live Tables in high-throughput streaming applications?
How would you set up a multi-region disaster recovery strategy for Delta Live Tables?

4. Databricks Performance Optimization Questions
How do you optimize Apache Spark jobs in Databricks?
What is Photon Engine, and how does it improve Databricks performance?
What are the best practices for autoscaling clusters in Databricks?
How do caching and data skipping improve query performance?
What is serverless compute in Databricks, and when should you use it?

5. Databricks Security and Governance Questions
How do you secure sensitive data in Databricks?
How does row-level security (RLS) work in Databricks?
What is Databricks Asset Bundles, and how does it improve security?
How do you manage service principal authentication in Databricks?
How can you enforce audit logging and compliance in Databricks?

6. Databricks ML and AI Integration Questions
How does Databricks support Machine Learning workflows?
What is MLflow, and how is it used in Databricks?
How do you train and deploy ML models in Databricks?
What is Feature Store in Databricks?
How do you integrate Azure Machine Learning with Databricks?

7. Real-World Scenario-Based Databricks Questions
Scenario 1: Retail Data Processing with Delta Lake
Question: A retail company wants to ingest real-time sales data, perform near real-time aggregations, and store the data in Delta Lake for downstream analytics. How would you design this pipeline?

Scenario 2: Multi-Cloud Data Sharing
Question: A company is using AWS, Azure, and GCP. They want to use Unity Catalog to manage cross-cloud data access securely. How would you implement it?

Scenario 3: Real-Time IoT Data Processing
Question: An energy company is collecting real-time IoT sensor data from thousands of locations and wants to use Delta Live Tables to process and store the data. What challenges would you anticipate, and how would you solve them?

Scenario 4: Handling Schema Evolution in Delta Lake
Question: A data engineering team is facing frequent schema changes in their incoming data. How can they automate schema evolution in Delta Lake while preventing data corruption?

Scenario 5: Cost Optimization in Databricks
Question: A company is running large-scale machine learning workloads in Databricks, and their costs are rising. What are the best practices to optimize compute and storage costs?
---------------------------------------------------------------------------------------------------------------------------

Data Lake
What is a Data Lake, and how does it differ from a Data Warehouse?

What are the key benefits of using a Data Lake?

What are the common challenges of managing a Data Lake?

How do you ensure data quality in a Data Lake?

What file formats are commonly used in Data Lakes (e.g., Parquet, ORC)?

How do you handle schema evolution in a Data Lake?

What is the role of metadata in a Data Lake?

How do you secure data in a Data Lake?

What are the best practices for organizing data in a Data Lake (e.g., partitioning, folder structure)?

How do you handle data ingestion in a Data Lake?

What is the difference between a Data Lake and a Data Lakehouse?

How do you optimize query performance in a Data Lake?

What tools can be used to process data in a Data Lake (e.g., Spark, Hive)?

How do you handle data retention and archival in a Data Lake?

What is the role of a Data Catalog in a Data Lake?
--------------------------------------------------------------------
Delta Lake
What is Delta Lake, and how does it enhance Data Lakes?

What are the key features of Delta Lake (e.g., ACID transactions, schema enforcement)?

How does Delta Lake handle schema evolution?

What is the difference between Delta Lake and traditional Data Lakes?

How do you create a Delta Lake table?

What is the difference between DELETE, UPDATE, and MERGE in Delta Lake?

How do you perform time travel in Delta Lake?

What is the purpose of the VACUUM command in Delta Lake?

How do you optimize Delta Lake tables (e.g., Z-Ordering, compaction)?

What is the role of the Delta Log in Delta Lake?

How do you handle concurrent writes in Delta Lake?

What are the best practices for partitioning data in Delta Lake?

How do you handle data deduplication in Delta Lake?

What is the difference between Delta Lake and Delta Live Tables?

How do you integrate Delta Lake with Apache Spark?
-------------------------------------------------------------------
Delta Live Tables (DLT)
What are Delta Live Tables, and how do they simplify ETL pipelines?

What are the key features of Delta Live Tables (e.g., declarative pipelines, automatic scaling)?

How do you create a Delta Live Table pipeline?

What is the difference between LIVE and STREAMING tables in DLT?

How do you handle schema enforcement in Delta Live Tables?

What are the best practices for monitoring Delta Live Table pipelines?

How do you handle incremental data processing in DLT?

What is the role of the APPLY CHANGES API in DLT?

How do you handle data quality checks in Delta Live Tables?

What are the advantages of using Delta Live Tables over traditional ETL tools?
---------------------------------------------------------------------------
Unity Catalog
What is Unity Catalog, and what problem does it solve?

What are the key features of Unity Catalog (e.g., centralized governance, fine-grained access control)?

How do you enable Unity Catalog in Databricks?

What is the difference between Unity Catalog and traditional Data Catalogs?

How do you define and manage data access policies in Unity Catalog?

What is the role of metastores in Unity Catalog?

How do you integrate Unity Catalog with Delta Lake?

What are the best practices for data lineage tracking in Unity Catalog?

How do you handle data sharing across organizations using Unity Catalog?

What are the limitations of Unity Catalog, and how can they be addressed?
----------------------------------------------------------------
Bonus Questions (Practical Scenarios)
How do you migrate from a traditional Data Lake to Delta Lake?

How do you handle data versioning and rollback in Delta Lake?

How do you design a Data Lakehouse architecture using Delta Lake and Unity Catalog?

How do you troubleshoot performance issues in Delta Live Tables?

How do you enforce GDPR compliance using Unity Catalog?
-----------------------------------------------------------------------------------------------------

 PySpark Basics and RDDs
 Q1. What is the difference between RDD, DataFrame, and Dataset?
 Q2. How does PySpark achieve parallel processing?
 Q3. Explain lazy evaluation in PySpark with a real-world analogy.
 Q4. What is SparkContext, and why is it important?
 Q5. How do you handle large file processing in PySpark?
 Q6. What is the difference between actions and transformations inPySpark?
 Q7. How does Spark handle data partitioning in distributed environments?
 Q8. Explain the concept of fault tolerance in PySpark.
 Q9. How do you broadcast variables in Spark, and when should you use them?
 Q10. What are accumulators in PySpark, and how do they differ from broadcast variables?

 DataFrame and Dataset Operations
 Q11. How do you perform data filtering using PySpark DataFrames?
 Q12. What is the difference between repartition() and coalesce(), and when would you use each?
 Q13. How do you handle missing or null values in PySpark?
 Q14. How can you add a new column to a DataFrame using withColumn()?
 Q15. How do you perform a left join between two DataFrames in PySpark?
 Q16. What are temporary views in PySpark, and how do they differ from global temporary views?
 Q17. How do you use window functions in PySpark for advanced analytics?
 Q18. How can you register a UDF (User-Defined Function) in PySpark?
 Q19. What is the difference between persist() and cache()?
 Q20. How do you read and write data in Parquet, CSV, and JSON formats in PySpark?

Spark SQL and Query Optimization
 Q21. How do you run SQL queries on a DataFrame in PySpark?
 Q22. What is the purpose of Catalyst Optimizer in Spark SQL?
 Q23. How do you handle schema inference when reading data from external sources?
 Q24. What are the different join types in Spark SQL, and when would you use each?
 Q25. How do you create a persistent table in Spark SQL?
 Q26. How does dynamic partition pruning improve query performance?
 Q27. Explain how to use broadcast joins to optimize query performance.
 Q28. What is data skew, and how do you handle it in Spark SQL?
 Q29. How can you perform aggregations using SQL queries on large datasets?
 Q30. How do you enable query caching in Spark SQL?

Data Pipeline Scenarios and Real
World Use Cases
 Q31. How would you build an ETL pipeline using PySpark?
 Q32. How do you handle real-time data processing with Structured Streaming in PySpark?
 Q33. What are the best practices for partitioning data in large datasets?
 Q34. How would you debug and optimize a slow-running Spark job?
 Q35. How do you handle schema evolution in PySpark pipelines?
 Q36. What is the role of checkpointing in Spark Streaming?
 Q37. How can you implement incremental data processing in PySpark?
 Q38. How do you handle large joins between multiple DataFrames?
 Q39. What is the difference between batch processing and stream processing in Spark?
 Q40. How would you secure sensitive data in a PySpark pipeline?

 Advanced PySpark Features
 Q41. How do you handle large datasets in PySpark to optimize performance and reduce memory usage?
 Q42. What is the purpose of Delta Lake, and how does it improve reliability?
 Q43. How do you enable time travel queries using Delta Lake?
 Q44. How do you handle complex aggregations using window functions?
 Q45. What are stateful operations in Spark Structured Streaming?
 Q46. How do you implement error handling and retries in PySpark jobs?
 Q47. How do you monitor and manage Spark clusters using Spark UI?
 Q48. What is the difference between SparkSession and SparkContext?
 Q49. How do you handle late-arriving data in Spark Structured Streaming?
 Q50. What is the difference between Spark’s Catalyst Optimizer and Tungsten Execution Engine?
-------------------

 Bonus: Practical Coding Challenges
 💻
 Challenge 1: Write a PySpark function to remove duplicate rows from a DataFrame based on specific columns.
 💻
 Challenge 2: Create a PySpark pipeline to read a CSV file, filter out rows with null values, and write the result to a Parquet file.
 💻
 Challenge 3: Implement a window function to rank salespeople based on total sales by region.
 💻
 Challenge 4: Write a PySpark SQL query to calculate the average salary by department, including only employees with more than 3 years of experience.
 💻
 Challenge 5: Implement a PySpark function to split a large DataFrame into smaller DataFrames based on a specific column value.

------------------

------------------

Basic Questions:
1. What is Delta Lake, and how does it differ from traditional data lakes?

Delta Lake is an open-source storage layer that brings reliability to data lakes by providing ACID transactions, scalable metadata handling, and data versioning. It sits on top of existing data lakes (e.g., S3, ADLS, HDFS) and enhances them with features like schema enforcement, schema evolution, and time travel.

Differences from traditional data lakes:

ACID Transactions: Delta Lake ensures atomicity, consistency, isolation, and durability, which traditional data lakes lack.

Schema Enforcement: Delta Lake enforces schema on write, preventing data corruption due to schema mismatches.

Time Travel: Allows querying historical versions of data, which is not natively supported in traditional data lakes.

Data Versioning: Delta Lake maintains a transaction log to track changes, enabling rollback and auditability.

2. What are the key features of Delta Lake?

ACID Transactions: Ensures data integrity in multi-user environments.

Schema Enforcement and Evolution: Prevents bad data from being written and allows schema changes over time.

Time Travel: Query historical data versions.

Scalable Metadata Handling: Efficiently manages metadata for large datasets.

Data Versioning: Tracks changes and allows rollback to previous versions.

Optimizations: Features like Z-Ordering, data skipping, and compaction improve query performance.

3. How does Delta Lake handle ACID transactions in big data processing?

Delta Lake uses a transaction log (Delta Log) to record all changes made to the data. Each transaction is assigned a unique version number, ensuring atomicity and consistency.

Isolation: Delta Lake provides snapshot isolation, meaning readers see a consistent snapshot of the data as of the start of their query, even if writes are happening concurrently.

Durability: Once a transaction is committed, it is written to the transaction log and cannot be lost.

4. What is schema enforcement, and how does it work in Delta Lake?

Schema Enforcement: Ensures that data being written to a Delta table adheres to the defined schema.

How it works: When data is written, Delta Lake checks the schema of the incoming data against the table's schema. If there’s a mismatch (e.g., extra columns, wrong data types), the write operation fails, preventing data corruption.

5. What is schema evolution in Delta Lake? How do you handle schema changes?

Schema Evolution: Allows the schema of a Delta table to change over time (e.g., adding new columns, changing data types).

Handling Schema Changes:

Add Columns: New columns can be added, and existing data will have NULL values for the new columns.

Change Data Types: Delta Lake supports certain type changes (e.g., INT to LONG), but some changes may require rewriting the data.

Schema Merging: When appending data with a different schema, Delta Lake can merge schemas automatically or throw an error if the merge is not possible.

Intermediate Questions:
6. How does Delta Lake store data internally? Explain the role of Parquet and transaction logs.

Data Storage: Delta Lake stores data in Parquet files, which are columnar and optimized for performance.

Transaction Log (Delta Log): Tracks all changes to the data, including inserts, updates, and deletes. It maintains a record of each transaction, enabling ACID properties and time travel.

7. What is Time Travel in Delta Lake, and how do you use it?

Time Travel: Allows querying historical versions of data by specifying a version number or timestamp.

Usage:

Query a specific version: SELECT * FROM table_name VERSION AS OF 5

Query a specific timestamp: SELECT * FROM table_name TIMESTAMP AS OF '2023-10-01'

8. How does Delta Lake optimize queries using Z-Ordering and data skipping?

Z-Ordering: Co-locates related information in the same set of files by sorting data based on one or more columns. This improves query performance by reducing the amount of data scanned.

Data Skipping: Delta Lake uses statistics (e.g., min/max values) stored in the transaction log to skip irrelevant files during query execution.

9. What is the role of OPTIMIZE and VACUUM in Delta Lake?

OPTIMIZE: Compacts small files into larger ones, improving query performance by reducing the number of files to scan.

VACUUM: Removes old files that are no longer needed (e.g., after data deletion or compaction). It helps manage storage costs but should be used cautiously to avoid losing time travel capabilities.

10. Explain how Delta Lake prevents data corruption in a multi-user environment.

Delta Lake uses ACID transactions and snapshot isolation to ensure that concurrent reads and writes do not interfere with each other.

The transaction log ensures that all changes are recorded and applied atomically, preventing partial updates or data corruption.

Advanced Questions (Real-World Scenarios):

11. How would you incrementally load data into a Delta table?
Use MERGE or INSERT operations with a condition to check for new or updated records.
Example:

MERGE INTO target_table AS target USING source_table AS source ON target.id = source.id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *

12. How does Merge, Update, and Delete work in Delta Lake?

MERGE: Combines insert, update, and delete operations in a single transaction. It is useful for upserts (update or insert).

UPDATE: Modifies existing records based on a condition.

DELETE: Removes records based on a condition.

All operations are recorded in the transaction log, ensuring ACID compliance.

13. What are the differences between append-only tables and mergeable tables in Delta Lake?

Append-Only Tables: Only allow inserts; updates and deletes are not permitted. Useful for immutable data (e.g., logs).

Mergeable Tables: Support inserts, updates, and deletes. Provide more flexibility but require more management (e.g., compaction, vacuuming).

14. How do you automate the compaction of small files in Delta Lake?

Use the OPTIMIZE command periodically or schedule it using a workflow tool (e.g., Apache Airflow, Databricks Jobs).

Example:
OPTIMIZE table_name

15. What challenges can arise when using Delta Lake in streaming pipelines, and how do you overcome them?
Challenges:
Small Files: Streaming can create many small files, impacting query performance.
Latency: Balancing between frequent writes and query performance.
Schema Changes: Handling schema evolution in real-time.

Solutions:
Use OPTIMIZE to compact small files.

Tune the streaming checkpoint interval to balance latency and performance.
Use schema evolution features to handle schema changes gracefully.

--
Basic Questions:
1. What is Unity Catalog, and why is it used in Databricks?

Unity Catalog is a unified governance solution for data and AI in Databricks. It provides centralized access control, auditing, and data lineage across workspaces, clouds, and data assets.

Why it’s used:
Centralized data governance for all data assets.
Fine-grained access control (e.g., row-level, column-level).
Data lineage for tracking data flow and transformations.
Cross-workspace data sharing and collaboration.
2. How does Unity Catalog improve data governance in Databricks?

Centralized Metadata Management: Provides a single source of truth for metadata across all workspaces.
Fine-Grained Access Control: Supports table, row, and column-level permissions.
Auditing and Compliance: Tracks data access and changes for auditing purposes.
Data Lineage: Visualizes how data flows and transforms across pipelines.
Cross-Workspace Sharing: Enables secure data sharing across different Databricks workspaces.

3. What are the different privileges and permissions available in Unity Catalog?

Privileges:

SELECT: Read data from a table.
MODIFY: Insert, update, or delete data.
CREATE: Create tables or databases.
USAGE: Access a database or table.
OWNERSHIP: Full control over a table or database.
Permissions: Applied at the database, table, or column level using Role-Based Access Control (RBAC).

4. How does RBAC (Role-Based Access Control) work in Unity Catalog?

RBAC allows administrators to assign roles to users or groups, and these roles define what actions they can perform.

Example:

Create roles like data_analyst, data_engineer, and data_scientist.
Assign privileges (e.g., SELECT, MODIFY) to these roles.
Assign users or groups to the roles.

5. What is the difference between Unity Catalog and Hive Metastore?

Unity Catalog:
Centralized governance across multiple workspaces and clouds.
Fine-grained access control (row-level, column-level).
Data lineage and auditing.
Cross-workspace data sharing.

Hive Metastore:
Limited to a single workspace.
Basic table-level access control.
No built-in data lineage or auditing.
No cross-workspace sharing.

Intermediate Questions:
6. How do you enable cross-workspace data sharing in Unity Catalog?

Use Delta Sharing to securely share data across workspaces or organizations.

Steps:

Create a share in the source workspace.
Add tables or databases to the share.
Grant access to the share for the target workspace or external users.
Access the shared data in the target workspace using a recipient token.

7. What are managed tables vs. external tables in Unity Catalog?

Managed Tables:

Databricks manages the storage and lifecycle of the data.
Data is stored in the workspace’s default storage location.
Dropping the table deletes the underlying data.

External Tables:
Data is stored in an external location (e.g., S3, ADLS).
Databricks only manages the metadata.
Dropping the table does not delete the underlying data.

8. How does column-level security work in Unity Catalog?

Column-level security restricts access to specific columns in a table.
Example:
Grant SELECT privilege on specific columns to a role.
Users in that role can only query the allowed columns.

Syntax:
GRANT SELECT (column1, column2) ON TABLE table_name TO ROLE role_name;

9. What is Data Lineage, and how does Unity Catalog track it?
Data Lineage: Tracks the flow of data from source to destination, including transformations.

Unity Catalog tracks lineage by:
Capturing metadata about tables, columns, and transformations.
Visualizing dependencies between datasets and jobs.
Providing insights into how data is used and transformed.

10. How do Service Principals and Personal Access Tokens (PATs) work with Unity Catalog?
Service Principals: Used for machine-to-machine authentication, allowing automated workflows to access Unity Catalog.
Personal Access Tokens (PATs): Used for user authentication, allowing users to access Unity Catalog programmatically.
Both are used to authenticate API requests or integrations with Unity Catalog.

Advanced Questions (Real-World Scenarios):
11. How do you set up fine-grained access control for different user roles in Unity Catalog?

Define roles (e.g., analyst, engineer, admin).
Assign privileges to roles (e.g., SELECT, MODIFY).
Assign users or groups to roles.

Example:
CREATE ROLE analyst;
GRANT SELECT ON TABLE sales TO ROLE analyst;
GRANT ROLE analyst TO USER user@example.com;

12. How can Unity Catalog be integrated with Azure Purview for data governance?
Use Azure Purview as a centralized data governance solution.

Steps:
Register Databricks workspaces in Azure Purview.
Scan Unity Catalog metadata into Purview.
Use Purview for data classification, lineage, and policy enforcement.
Sync Unity Catalog’s access controls with Purview’s policies.

13. How would you migrate from Hive Metastore to Unity Catalog?

Steps:
Assess existing metadata and permissions in Hive Metastore.
Create corresponding databases, tables, and permissions in Unity Catalog.
Migrate data to Unity Catalog-managed or external tables.
Update jobs and workflows to use Unity Catalog.
Validate access controls and data lineage.

14. How do you enforce data masking in Unity Catalog?

Use dynamic views or column-level security to mask sensitive data.
Example:
Create a view that masks sensitive columns:

CREATE VIEW masked_view AS
SELECT name, mask(ssn) AS ssn FROM sensitive_table;
Grant access to the view instead of the underlying table.

15. How does data versioning work with Unity Catalog?

Unity Catalog leverages Delta Lake’s versioning capabilities.
Each change to a table (e.g., insert, update, delete) creates a new version.
Use Time Travel to query historical versions of the data.

Example:
SELECT * FROM table_name VERSION AS OF 5;

----
Delta Live Tables (DLT) Interview Questions
Basic Questions:
1. What is Delta Live Tables (DLT) in Databricks?

Delta Live Tables (DLT) is a framework for building reliable, maintainable, and scalable data pipelines on Databricks. It simplifies ETL/ELT development by providing declarative APIs for defining data transformations and automating pipeline orchestration, monitoring, and maintenance.

2. How does Delta Live Tables differ from normal Delta Tables?

Delta Live Tables:
Provides a declarative API for defining pipelines.
Automates orchestration, monitoring, and maintenance.
Supports both streaming and batch processing.
Enforces data quality constraints.

Normal Delta Tables:
Require manual orchestration and monitoring.
Do not provide built-in data quality enforcement.
Require custom logic for streaming and batch processing.

3. What are the benefits of using Delta Live Tables for streaming and batch processing?

Declarative Pipelines: Simplifies ETL/ELT development.
Automated Maintenance: Handles tasks like compaction, vacuuming, and optimization.
Data Quality Enforcement: Ensures data integrity with constraints.
Unified Processing: Supports both streaming and batch workloads.
Scalability: Automatically scales to handle large datasets.

4. What is the difference between STREAMING and INCREMENTAL tables in Delta Live Tables?

STREAMING Tables:
Designed for continuous data ingestion.
Use Delta Lake’s streaming capabilities.
Process data in real-time as it arrives.

INCREMENTAL Tables:
Designed for batch processing with incremental updates.
Process only new or changed data since the last run.
Optimized for periodic batch jobs.

5. How do data quality constraints work in Delta Live Tables?
Data Quality Constraints:
Define rules (e.g., NOT NULL, UNIQUE) to ensure data integrity.
Enforced during pipeline execution.

Example:
@dlt.table
def filtered_data():
    return spark.read.format("delta").load("/path/to/data")
        .filter("column IS NOT NULL")
Violations can be logged, quarantined, or cause pipeline failures.

Intermediate Questions:
6. How does Auto-Maintenance work in Delta Live Tables?

Auto-Maintenance:
Automatically handles tasks like compaction, vacuuming, and optimization.
Ensures optimal performance and storage efficiency.
Example: Compacts small files into larger ones to improve query performance.

7. What is the role of Pipelines in Delta Live Tables?

Pipelines:
Define the flow of data transformations.
Consist of tables and views defined using DLT APIs.
Automatically orchestrate and execute the pipeline.

Example:
dlt.create_streaming_table("raw_data")
dlt.apply_changes("cleaned_data", source="raw_data", keys=["id"])

8. How do you enable Change Data Capture (CDC) using Delta Live Tables?

Use the apply_changes API to handle CDC.

Example:
dlt.create_streaming_table("target_table")
dlt.apply_changes(
    target="target_table",
    source="source_table",
    keys=["id"],
    sequence_by="updated_at",
    apply_as_deletes="operation = 'DELETE'")

9. How do you monitor pipeline failures and logs in Delta Live Tables?

Use the DLT UI in Databricks to monitor pipeline runs.
View logs, metrics, and failure details.
Set up alerts for pipeline failures using Databricks Jobs or external monitoring tools.

10. How can Delta Live Tables be integrated with Unity Catalog?

Integration Steps:
Enable Unity Catalog in the Databricks workspace.
Use Unity Catalog’s metadata and access controls for DLT tables.
Define DLT pipelines using Unity Catalog-managed tables.
Enforce fine-grained access control using Unity Catalog.

Advanced Questions (Real-World Scenarios):
11. How would you implement a real-time data ingestion pipeline using Delta Live Tables?

Steps:
Define a STREAMING table for raw data ingestion.
Apply transformations using DLT APIs.
Enforce data quality constraints.
Write the processed data to a Delta table.

Example:

@dlt.table
def raw_data():
    return spark.readStream.format("kafka").load()

@dlt.table
def processed_data():
    return dlt.read_stream("raw_data").filter("value IS NOT NULL")

12. How does Delta Live Tables handle schema evolution in a production environment?

Schema Evolution:
Automatically handles schema changes (e.g., adding columns).
Supports merging schemas when appending data.

Example:
dlt.create_table("evolving_table", schema="id INT, name STRING")
dlt.append("evolving_table", new_data_with_new_columns)

13. How do you optimize cost and performance in Delta Live Tables?

Optimization Strategies:
Use Auto-Maintenance for compaction and optimization.
Enable data skipping and Z-Ordering.
Use INCREMENTAL tables for batch processing.
Monitor and tune cluster configurations.

14. What are the challenges of using Delta Live Tables in high-throughput streaming applications?

Challenges:
Handling backpressure from high data volumes.
Ensuring low-latency processing.
Managing small file problems in streaming.

Solutions:
Use Auto-Maintenance for compaction.
Optimize cluster resources and parallelism.
Monitor pipeline performance and scale resources as needed.

15. How would you set up a multi-region disaster recovery strategy for Delta Live Tables?

Steps:
Replicate data to a secondary region using cloud storage replication (e.g., S3 Cross-Region Replication).
Set up a secondary Databricks workspace in the backup region.
Use Delta Lake’s Time Travel to recover data in case of failures.
Automate pipeline failover using orchestration tools (e.g., Databricks Jobs, Airflow).

-------------------------------------------
Real-World Scenario-Based Databricks Questions
Scenario 1: Retail Data Processing with Delta Lake
Question: A retail company wants to ingest real-time sales data, perform near real-time aggregations, and store the data in Delta Lake for downstream analytics. How would you design this pipeline?

Solution:
Data Ingestion:
Use Apache Kafka or Databricks Auto Loader to ingest real-time sales data.

Example:
df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "broker:9092").load()
Data Transformation:

Perform near real-time aggregations (e.g., sales by product, region) using Structured Streaming.

Example:
aggregated_df = df.groupBy("product_id", "region").agg(sum("sales").alias("total_sales"))

Data Storage:
Write the aggregated data to Delta Lake for reliability and ACID transactions.

Example:
aggregated_df.writeStream.format("delta").outputMode("update").option("checkpointLocation", "/path/to/checkpoint").start("/path/to/delta_table")
Downstream Analytics:

Use Databricks SQL or BI tools to query the Delta Lake tables for analytics.

Scenario 2: Multi-Cloud Data Sharing
Question: A company is using AWS, Azure, and GCP. They want to use Unity Catalog to manage cross-cloud data access securely. How would you implement it?

Solution:

Enable Unity Catalog:
Set up Unity Catalog in the Databricks workspace for each cloud provider.

Centralized Metadata Management:
Use Unity Catalog to manage metadata across all clouds in a centralized manner.

Cross-Cloud Data Sharing:
Use Delta Sharing to securely share data across clouds.

Example:

Create a share in the source workspace (e.g., AWS).
Add tables to the share.
Grant access to the share for target workspaces (e.g., Azure, GCP).

Fine-Grained Access Control:
Use Unity Catalog’s RBAC to enforce access controls at the table, row, or column level.

Auditing and Compliance:
Use Unity Catalog’s auditing features to track data access and changes across clouds.

Scenario 3: Real-Time IoT Data Processing
Question: An energy company is collecting real-time IoT sensor data from thousands of locations and wants to use Delta Live Tables to process and store the data. What challenges would you anticipate, and how would you solve them?

Challenges:

High Data Volume:
Solution: Use Auto-Scaling clusters to handle the load.

Data Latency:
Solution: Optimize streaming pipelines for low-latency processing.

Small File Problem:
Solution: Use Auto-Maintenance in Delta Live Tables to compact small files.

Data Quality:
Solution: Enforce data quality constraints in Delta Live Tables.

Implementation:
Data Ingestion:
Use Apache Kafka or Databricks Auto Loader to ingest IoT data.

Data Transformation:
Use Delta Live Tables to define transformations and aggregations.

Example:
@dlt.table
def raw_iot_data():
    return spark.readStream.format("kafka").load()

@dlt.table
def processed_iot_data():
    return dlt.read_stream("raw_iot_data").filter("value IS NOT NULL")
Data Storage:

Write processed data to Delta Lake for reliability and ACID transactions.

Scenario 4: Handling Schema Evolution in Delta Lake
Question: A data engineering team is facing frequent schema changes in their incoming data. How can they automate schema evolution in Delta Lake while preventing data corruption?

Solution:
Schema Enforcement:
Use Delta Lake’s schema enforcement to prevent bad data from being written.
Example:

df.write.format("delta").mode("append").option("mergeSchema", "true").save("/path/to/delta_table")
Schema Evolution:

Enable mergeSchema to automatically evolve the schema when new columns are added.

Example:
df.write.format("delta").mode("append").option("mergeSchema", "true").save("/path/to/delta_table")

Data Validation:
Use data quality constraints to validate incoming data.

Example:
@dlt.table
def validated_data():
    return spark.read.format("delta").load("/path/to/delta_table").filter("column IS NOT NULL")
Audit and Rollback:

Use Delta Lake’s Time Travel to audit changes and roll back if necessary.

Scenario 5: Cost Optimization in Databricks
Question: A company is running large-scale machine learning workloads in Databricks, and their costs are rising. What are the best practices to optimize compute and storage costs?

Solution:

Compute Optimization:
Use Spot Instances for non-critical workloads.
Right-size clusters to match workload requirements.
Use Auto-Scaling to scale resources dynamically.

Storage Optimization:
Use Delta Lake for efficient storage and query performance.
Enable Z-Ordering and Data Skipping to reduce query costs.
Use VACUUM to remove old files and reduce storage costs.

Job Optimization:
Schedule jobs during off-peak hours to take advantage of lower cloud costs.
Use Delta Live Tables to automate maintenance tasks like compaction.

Monitoring and Governance:
Use Databricks Cost Management tools to monitor and optimize costs.
Enforce cost controls using Unity Catalog’s access controls.

Machine Learning Optimization:
Use MLflow to track and optimize model training costs.
Use Databricks Runtime for ML for optimized machine learning workloads

--------------------------------

Data Lake
What is a Data Lake, and how does it differ from a Data Warehouse?
Data Lake: A centralized repository that stores structured, semi-structured, and unstructured data in its raw format. It is schema-on-read, meaning the schema is applied when the data is read.
Data Warehouse: A structured repository optimized for querying and analysis. It uses schema-on-write, meaning the schema is defined before data is loaded.

What are the key benefits of using a Data Lake?
Scalability to store large volumes of data.
Flexibility to store raw data in any format.
Cost-effective storage for big data.
Supports advanced analytics, machine learning, and AI.

What are the common challenges of managing a Data Lake?
Data quality issues (e.g., inconsistent data).
Lack of governance and metadata management.
Difficulty in querying unstructured data.
Risk of becoming a "data swamp" without proper organization.

How do you ensure data quality in a Data Lake?
Implement data validation rules during ingestion.
Use metadata to track data lineage and provenance.
Regularly audit and clean data.
Use tools like Apache Griffin or Deequ for data quality checks.

What file formats are commonly used in Data Lakes (e.g., Parquet, ORC)?
Parquet: Columnar format optimized for query performance.
ORC: Columnar format with better compression than Parquet.
Avro: Row-based format for schema evolution.
JSON/CSV: Common for semi-structured and unstructured data.

How do you handle schema evolution in a Data Lake?
Use file formats like Parquet or Avro that support schema evolution.
Implement schema-on-read to handle changes in data structure.
Use metadata to track schema changes over time.

What is the role of metadata in a Data Lake?
Tracks data lineage, schema, and ownership.
Enables data discovery and governance.
Helps in query optimization and data quality management.

How do you secure data in a Data Lake?
Use encryption for data at rest and in transit.
Implement role-based access control (RBAC).
Use tools like Apache Ranger or AWS Lake Formation for fine-grained access control.

What are the best practices for organizing data in a Data Lake (e.g., partitioning, folder structure)?

Use a hierarchical folder structure (e.g., /raw/, /processed/, /analytics/).
Partition data by date, region, or other relevant dimensions.
Use naming conventions for files and folders.

How do you handle data ingestion in a Data Lake?
Use batch ingestion tools like Apache NiFi or AWS Glue.
Use streaming tools like Apache Kafka or AWS Kinesis for real-time data.
Validate and clean data during ingestion.

What is the difference between a Data Lake and a Data Lakehouse?
Data Lake: Stores raw data without built-in ACID transactions or schema enforcement.
Data Lakehouse: Combines the scalability of a Data Lake with the reliability and performance of a Data Warehouse (e.g., Delta Lake).

How do you optimize query performance in a Data Lake?
Use columnar file formats like Parquet or ORC.
Partition data to reduce query scan size.
Use data skipping and indexing techniques.

What tools can be used to process data in a Data Lake (e.g., Spark, Hive)?
Apache Spark: For batch and stream processing.
Apache Hive: For SQL-based querying.
Presto/Trino: For interactive queries.

How do you handle data retention and archival in a Data Lake?

Define retention policies based on regulatory requirements.
Use tiered storage (e.g., hot, warm, cold) to manage costs.
Archive old data to cheaper storage like AWS Glacier.

What is the role of a Data Catalog in a Data Lake?
Provides a searchable inventory of data assets.
Tracks metadata, lineage, and ownership.
Enables data discovery and governance.

Delta Lake
What is Delta Lake, and how does it enhance Data Lakes?
Delta Lake is an open-source storage layer that adds reliability (ACID transactions), schema enforcement, and data versioning to Data Lakes.

What are the key features of Delta Lake (e.g., ACID transactions, schema enforcement)?
ACID transactions.
Schema enforcement and evolution.
Time travel and data versioning.
Scalable metadata handling.

How does Delta Lake handle schema evolution?
Automatically merges schemas when new columns are added.
Supports adding, renaming, and reordering columns.

What is the difference between Delta Lake and traditional Data Lakes?
Delta Lake provides ACID transactions, schema enforcement, and time travel, which traditional Data Lakes lack.

How do you create a Delta Lake table?

Example:
CREATE TABLE delta_table USING DELTA LOCATION '/path/to/delta_table';
What is the difference between DELETE, UPDATE, and MERGE in Delta Lake?
DELETE: Removes rows based on a condition.
UPDATE: Modifies existing rows.
MERGE: Combines insert, update, and delete operations in a single transaction.

How do you perform time travel in Delta Lake?
Query historical versions using version or timestamp:

SELECT * FROM delta_table VERSION AS OF 5;
SELECT * FROM delta_table TIMESTAMP AS OF '2023-10-01';
What is the purpose of the VACUUM command in Delta Lake?
Removes old files that are no longer needed, reducing storage costs.

How do you optimize Delta Lake tables (e.g., Z-Ordering, compaction)?
Use Z-Ordering to colocate related data.
Use OPTIMIZE to compact small files.

What is the role of the Delta Log in Delta Lake?
Tracks all changes to the data, enabling ACID transactions and time travel.

How do you handle concurrent writes in Delta Lake?
Delta Lake uses optimistic concurrency control to handle concurrent writes.

What are the best practices for partitioning data in Delta Lake?
Partition by frequently queried columns (e.g., date, region).
Avoid over-partitioning to prevent small file problems.

How do you handle data deduplication in Delta Lake?
Use the MERGE command to deduplicate data:

MERGE INTO target_table USING source_table ON target_table.id = source_table.id
WHEN NOT MATCHED THEN INSERT *;

What is the difference between Delta Lake and Delta Live Tables?
Delta Lake: A storage layer for reliable Data Lakes.
Delta Live Tables (DLT): A framework for building ETL pipelines using Delta Lake.

How do you integrate Delta Lake with Apache Spark?
Use the Delta Lake library in Spark:
df.write.format("delta").save("/path/to/delta_table")
Delta Live Tables (DLT)

What are Delta Live Tables, and how do they simplify ETL pipelines?
DLT is a declarative framework for building ETL pipelines with built-in reliability, scalability, and automation.

What are the key features of Delta Live Tables (e.g., declarative pipelines, automatic scaling)?
Declarative pipeline definitions.
Automatic scaling and maintenance.
Built-in data quality enforcement.
How do you create a Delta Live Table pipeline?

Example:
@dlt.table
def raw_data():
    return spark.read.format("csv").load("/path/to/data")
What is the difference between LIVE and STREAMING tables in DLT?

LIVE: Batch processing.

STREAMING: Continuous processing for real-time data.

How do you handle schema enforcement in Delta Live Tables?
Define schemas explicitly and use data quality constraints.

What are the best practices for monitoring Delta Live Table pipelines?
Use the DLT UI to monitor pipeline runs and logs.
Set up alerts for failures.

How do you handle incremental data processing in DLT?
Use the INCREMENTAL keyword to process only new or changed data.

What is the role of the APPLY CHANGES API in DLT?
Handles Change Data Capture (CDC) by merging changes into a target table.

How do you handle data quality checks in Delta Live Tables?
Use constraints to enforce data quality:

@dlt.expect("valid_id", "id IS NOT NULL")

What are the advantages of using Delta Live Tables over traditional ETL tools?
Simplified pipeline development.
Built-in reliability and scalability.
Automated maintenance and monitoring.

Unity Catalog
What is Unity Catalog, and what problem does it solve?

Unity Catalog provides centralized governance for data and AI assets across Databricks workspaces and clouds.

What are the key features of Unity Catalog (e.g., centralized governance, fine-grained access control)?
Centralized metadata management.
Fine-grained access control (row-level, column-level).
Data lineage and auditing.

How do you enable Unity Catalog in Databricks?
Enable it in the Databricks workspace settings and configure a metastore.

What is the difference between Unity Catalog and traditional Data Catalogs?
Unity Catalog provides fine-grained access control and cross-workspace governance.

How do you define and manage data access policies in Unity Catalog?
Use roles and privileges to define access policies:
GRANT SELECT ON TABLE table_name TO ROLE role_name;

What is the role of metastores in Unity Catalog?
Metastores store metadata for tables, databases, and schemas.

How do you integrate Unity Catalog with Delta Lake?
Use Unity Catalog to manage metadata and access controls for Delta Lake tables.

What are the best practices for data lineage tracking in Unity Catalog?
Enable lineage tracking for all pipelines.
Use lineage to audit data flows and transformations.

How do you handle data sharing across organizations using Unity Catalog?
Use Delta Sharing to securely share data across organizations.
What are the limitations of Unity Catalog, and how can they be addressed?
Limited to Databricks environments. Use Delta Sharing for cross-platform sharing.

Bonus Questions (Practical Scenarios)

How do you migrate from a traditional Data Lake to Delta Lake?
Assess existing data and metadata.
Convert data to Delta format using CONVERT TO DELTA.
Update pipelines to use Delta Lake.

How do you handle data versioning and rollback in Delta Lake?
Use Time Travel to query historical versions.
Rollback using RESTORE:
RESTORE TABLE delta_table TO VERSION AS OF 5;

How do you design a Data Lakehouse architecture using Delta Lake and Unity Catalog?
Use Delta Lake for storage and reliability.
Use Unity Catalog for governance and access control.
Integrate with BI tools for analytics.

How do you troubleshoot performance issues in Delta Live Tables?
Check cluster configurations and scaling.
Optimize queries using Z-Ordering and data skipping.
Monitor pipeline logs for errors.

How do you enforce GDPR compliance using Unity Catalog?
Use fine-grained access control to restrict access to sensitive data.
Track data lineage to monitor data usage.
Implement data retention and deletion policies.

--------------------------------------------------------------------

PySpark Basics and RDDs
Q1. What is the difference between RDD, DataFrame, and Dataset?
RDD (Resilient Distributed Dataset):
Low-level API for distributed data processing.
Immutable, fault-tolerant collections of objects.
No schema or optimization.

DataFrame:
High-level API with schema and optimization.
Distributed collection of rows with named columns.
Built on top of RDDs.

Dataset:
Combines the benefits of RDDs and DataFrames.
Type-safe and optimized for performance.

Q2. How does PySpark achieve parallel processing?
PySpark divides data into partitions and processes them in parallel across a cluster of nodes.

Q3. Explain lazy evaluation in PySpark with a real-world analogy.
Lazy evaluation means transformations are not executed until an action is called.

Analogy: Planning a trip (transformations) but not actually traveling (action) until you decide to go.

Q4. What is SparkContext, and why is it important?
SparkContext: Entry point to interact with Spark. It manages cluster resources and coordinates tasks.

Q5. How do you handle large file processing in PySpark?
Use distributed processing by partitioning the file and processing it in parallel across the cluster.

Q6. What is the difference between actions and transformations in PySpark?

Transformations: Create a new RDD/DataFrame (e.g., map, filter).
Actions: Trigger computation and return results (e.g., count, collect).

Q7. How does Spark handle data partitioning in distributed environments?

Data is split into partitions, and each partition is processed on a separate node.

Q8. Explain the concept of fault tolerance in PySpark.
PySpark achieves fault tolerance through lineage. If a partition is lost, it can be recomputed using the lineage graph.

Q9. How do you broadcast variables in Spark, and when should you use them?
Use sc.broadcast() to send read-only variables to all nodes.
Use them for small, frequently accessed data to reduce network overhead.

Q10. What are accumulators in PySpark, and how do they differ from broadcast variables?
Accumulators: Variables used for aggregating values across tasks (e.g., counters).
Broadcast Variables: Read-only variables shared across nodes.

DataFrame and Dataset Operations
Q11. How do you perform data filtering using PySpark DataFrames?
Use the filter() or where() method:
df.filter(df["age"] > 30)

Q12. What is the difference between repartition() and coalesce(), and when would you use each?
repartition(): Increases or decreases partitions (shuffles data).
coalesce(): Reduces partitions without shuffling.

Q13. How do you handle missing or null values in PySpark?
Use na.drop() to remove rows with nulls or na.fill() to replace nulls.

Q14. How can you add a new column to a DataFrame using withColumn()?

Example:
df.withColumn("new_col", df["old_col"] * 2)

Q15. How do you perform a left join between two DataFrames in PySpark?

Example:
df1.join(df2, on="key", how="left")

Q16. What are temporary views in PySpark, and how do they differ from global temporary views?

Temporary Views: Scoped to a single SparkSession.
Global Temporary Views: Scoped to a Spark application (accessible across sessions).

Q17. How do you use window functions in PySpark for advanced analytics?

Example:
from pyspark.sql.window import Window
window = Window.partitionBy("department").orderBy("salary")
df.withColumn("rank", rank().over(window))

Q18. How can you register a UDF (User-Defined Function) in PySpark?

Example:
from pyspark.sql.functions import udf
udf_func = udf(lambda x: x * 2)
df.withColumn("new_col", udf_func(df["old_col"]))

Q19. What is the difference between persist() and cache()?
cache(): Persists data in memory with default storage level.
persist(): Allows specifying storage level (e.g., memory, disk).

Q20. How do you read and write data in Parquet, CSV, and JSON formats in PySpark?

Example:
df.write.parquet("path/to/parquet")
df.write.csv("path/to/csv")
df.write.json("path/to/json")
Spark SQL and Query Optimization

Q21. How do you run SQL queries on a DataFrame in PySpark?

Create a temporary view and run SQL:
df.createOrReplaceTempView("table")
spark.sql("SELECT * FROM table")

Q22. What is the purpose of Catalyst Optimizer in Spark SQL?
Optimizes query plans for better performance.

Q23. How do you handle schema inference when reading data from external sources?
Use inferSchema=True when reading data
spark.read.csv("path/to/file", inferSchema=True)

Q24. What are the different join types in Spark SQL, and when would you use each?
Inner Join: Returns matching rows.
Left Join: Returns all rows from the left table and matching rows from the right.
Right Join: Opposite of left join.
Full Outer Join: Returns all rows from both tables.

Q25. How do you create a persistent table in Spark SQL?
df.write.saveAsTable("table_name")

Q26. How does dynamic partition pruning improve query performance?
Skips irrelevant partitions during query execution.

Q27. Explain how to use broadcast joins to optimize query performance.
Broadcast smaller tables to all nodes to avoid shuffling:
df1.join(broadcast(df2), on="key")

Q28. What is data skew, and how do you handle it in Spark SQL?
Data Skew: Uneven distribution of data across partitions.
Solution: Use salting or repartitioning.

Q29. How can you perform aggregations using SQL queries on large datasets?

Example:
SELECT department, AVG(salary) FROM employees GROUP BY department;

Q30. How do you enable query caching in Spark SQL?

Use CACHE TABLE:
CACHE TABLE table_name;
Data Pipeline Scenarios and Real-World Use Cases

Q31. How would you build an ETL pipeline using PySpark?
Extract data from sources, transform it using PySpark, and load it into a target system.

Q32. How do you handle real-time data processing with Structured Streaming in PySpark?
Use readStream and writeStream APIs:
df = spark.readStream.format("kafka").load()
df.writeStream.format("delta").start()

Q33. What are the best practices for partitioning data in large datasets?
Partition by frequently queried columns (e.g., date).

Q34. How would you debug and optimize a slow-running Spark job?
Check Spark UI for bottlenecks, optimize shuffling, and use caching.

Q35. How do you handle schema evolution in PySpark pipelines?
Use mergeSchema=True when writing data.

Q36. What is the role of checkpointing in Spark Streaming?
Ensures fault tolerance by saving state to reliable storage.

Q37. How can you implement incremental data processing in PySpark?
Use merge or append mode to process only new data.

Q38. How do you handle large joins between multiple DataFrames?
Use broadcast joins for small tables and repartitioning for large tables.

Q39. What is the difference between batch processing and stream processing in Spark?
Batch Processing: Processes data in fixed intervals.
Stream Processing: Processes data in real-time.

Q40. How would you secure sensitive data in a PySpark pipeline?
Use encryption, access controls, and masking.

Advanced PySpark Features
Q41. How do you handle large datasets in PySpark to optimize performance and reduce memory usage?

Use partitioning, caching, and efficient data formats (e.g., Parquet).

Q42. What is the purpose of Delta Lake, and how does it improve reliability?

Adds ACID transactions, schema enforcement, and time travel to Data Lakes.

Q43. How do you enable time travel queries using Delta Lake?
Example:
SELECT * FROM delta_table VERSION AS OF 5;

Q44. How do you handle complex aggregations using window functions?
from pyspark.sql.window import Window
window = Window.partitionBy("department").orderBy("salary")
df.withColumn("rank", rank().over(window))

Q45. What are stateful operations in Spark Structured Streaming?
Operations that maintain state across batches (e.g., mapGroupsWithState).

Q46. How do you implement error handling and retries in PySpark jobs?

Use try-catch blocks and retry logic in driver code.

Q47. How do you monitor and manage Spark clusters using Spark UI?

Use the Spark UI to track job progress, resource usage, and bottlenecks.

Q48. What is the difference between SparkSession and SparkContext?

SparkSession: Entry point for DataFrame and SQL APIs.

SparkContext: Entry point for RDD APIs.

Q49. How do you handle late-arriving data in Spark Structured Streaming?
Use watermarking to handle late data:
df.withWatermark("timestamp", "10 minutes")

Q50. What is the difference between Spark’s Catalyst Optimizer and Tungsten Execution Engine?
Catalyst Optimizer: Optimizes query plans.
Tungsten Engine: Optimizes memory and CPU usage for execution.